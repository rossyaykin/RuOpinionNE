{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b357de-2dab-4074-81c8-9b2a662f393d",
   "metadata": {},
   "source": [
    "### Импорты ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e670f2-5a54-459b-94e2-b5c5ddb57a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T00:08:17.956503Z",
     "iopub.status.busy": "2024-11-20T00:08:17.955288Z",
     "iopub.status.idle": "2024-11-20T00:08:37.508867Z",
     "shell.execute_reply": "2024-11-20T00:08:37.507642Z",
     "shell.execute_reply.started": "2024-11-20T00:08:17.956453Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %pip install vllm\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from random import choices\n",
    "import ast\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcab233f-5c1f-41ed-bbe6-02f1c6c6393c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T00:10:01.408544Z",
     "iopub.status.busy": "2024-11-20T00:10:01.406696Z",
     "iopub.status.idle": "2024-11-20T00:10:01.830986Z",
     "shell.execute_reply": "2024-11-20T00:10:01.829606Z",
     "shell.execute_reply.started": "2024-11-20T00:10:01.408487Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "# download source files\n",
    "url = 'https://raw.githubusercontent.com/rossyaykin/RuOpinionNE/refs/heads/main/src/src.zip'\n",
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df5b4c-f263-4b75-98a5-aaf0febee197",
   "metadata": {},
   "source": [
    "### Загрузка данных ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412b2362-84be-45bd-8364-42a9f60e08d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T00:10:03.572299Z",
     "iopub.status.busy": "2024-11-20T00:10:03.570562Z",
     "iopub.status.idle": "2024-11-20T00:10:03.605086Z",
     "shell.execute_reply": "2024-11-20T00:10:03.603863Z",
     "shell.execute_reply.started": "2024-11-20T00:10:03.572254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.utils import load_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614f720e-e41b-4247-8938-f5386884d396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T00:10:40.016123Z",
     "iopub.status.busy": "2024-11-20T00:10:40.014748Z",
     "iopub.status.idle": "2024-11-20T00:10:41.252213Z",
     "shell.execute_reply": "2024-11-20T00:10:41.251128Z",
     "shell.execute_reply.started": "2024-11-20T00:10:40.016061Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556 512 1316\n"
     ]
    }
   ],
   "source": [
    "train_path = \"full.jsonl\"\n",
    "val_path = \"gold.jsonl\"\n",
    "test_path = \"validation.jsonl\"\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/rossyaykin/RuOpinionNE/refs/heads/main/data/full.jsonl'\n",
    "train = load_jsonl(url, train_path)\n",
    "url = 'https://raw.githubusercontent.com/rossyaykin/RuOpinionNE/refs/heads/main/data/gold.jsonl'\n",
    "val = load_jsonl(url, val_path)\n",
    "url = 'https://raw.githubusercontent.com/rossyaykin/RuOpinionNE/refs/heads/main/data/validation.jsonl'\n",
    "test = load_jsonl(url, test_path)\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ea2e9-e570-4305-a14b-b9383bbc1bc3",
   "metadata": {},
   "source": [
    "### Определения ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea2c11a-4941-42df-814a-7d70e83793f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T00:10:47.059121Z",
     "iopub.status.busy": "2024-11-20T00:10:47.057763Z",
     "iopub.status.idle": "2024-11-20T00:10:47.104079Z",
     "shell.execute_reply": "2024-11-20T00:10:47.102950Z",
     "shell.execute_reply.started": "2024-11-20T00:10:47.059085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.utils import dict2tuple, extract_tuple, form_prompt, str2list, short_report, df2structure\n",
    "from src.evaluation import do_eval_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8b03f2-86c9-4f8e-b4fd-30ced7795128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T00:10:48.602301Z",
     "iopub.status.busy": "2024-11-20T00:10:48.600489Z",
     "iopub.status.idle": "2024-11-20T00:10:48.636418Z",
     "shell.execute_reply": "2024-11-20T00:10:48.635332Z",
     "shell.execute_reply.started": "2024-11-20T00:10:48.602247Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(model, sampling_params, train, test, n_shots = 5, chat_template = False):\n",
    "    \"\"\"generate preds as a list of lists\"\"\"\n",
    "    \n",
    "    prompts = list()\n",
    "    for entry in test:\n",
    "        examples = [dict2tuple(x) for x in choices(train, k = n_shots)]\n",
    "        prompt = form_prompt(examples, entry['text'])\n",
    "        if chat_template:\n",
    "            prompt = llm.get_tokenizer().apply_chat_template([{\"role\": \"user\", \"content\": prompt}],\n",
    "                                                             tokenize=False,\n",
    "                                                             add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    output = llm.generate(prompts, sampling_params)\n",
    "    # (sent_id, text, target, pred)\n",
    "    result = [(test[i]['sent_id'],\n",
    "               test[i]['text'],\n",
    "               dict2tuple(test[i])[1],\n",
    "               extract_tuple(output[i].outputs[0].text)) for i in range(len(test))]\n",
    "    return result\n",
    "\n",
    "def get_path(model_name, sampling_params, n_shots, chat_template = False, short_path = False):\n",
    "    full_name = model_name.split('/')[1]\n",
    "    if short_path:\n",
    "        model_tag = ''\n",
    "        for c in full_name:\n",
    "            if not c.isalpha():\n",
    "                break\n",
    "            model_tag+=c\n",
    "    else:\n",
    "        model_tag = full_name\n",
    "    \n",
    "    n_shots, temp = str(n_shots), str(sampling_params.temperature)\n",
    "    path = f'results/{model_tag}/{model_tag}_bl_{n_shots}shot_{temp}temp'\n",
    "    if chat_template:\n",
    "        path+='_chat'\n",
    "    # returns full path but without \".csv\"\n",
    "    return path\n",
    "\n",
    "def to_jsonl(data, target):\n",
    "    \"\"\"takes a list of dicts and path;\n",
    "    saves the list to jsonl\"\"\"\n",
    "\n",
    "    with open(target, \"w\") as f:\n",
    "        for item in data:\n",
    "            f.write(f\"{json.dumps(item, ensure_ascii=False)}\\n\")\n",
    "\n",
    "def save(data, path):\n",
    "    outdir, outname = '/'.join(path.split('/')[:-1]), path.split('/')[-1]\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    to_jsonl(data, f'{path}.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a3152-8ca3-4e1c-9d84-5be37ae2fc39",
   "metadata": {},
   "source": [
    "### Инференс ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b68e84f-c0cb-457e-b1fa-e3278a56254e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T22:52:04.080904Z",
     "iopub.status.busy": "2024-11-14T22:52:04.079315Z",
     "iopub.status.idle": "2024-11-14T22:52:04.114501Z",
     "shell.execute_reply": "2024-11-14T22:52:04.113110Z",
     "shell.execute_reply.started": "2024-11-14T22:52:04.080855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai-forever/ruBert-large',\n",
       " 'ai-forever/ruElectra-large',\n",
       " 'DeepPavlov/rubert-base-cased-conversational',\n",
       " 'google-bert/bert-base-multilingual-cased',\n",
       " 'IlyaGusev/saiga_llama3_8b']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = \"Vikhrmodels/Vikhr-7B-instruct_0.4\"\n",
    "# model_name = \"Vikhrmodels/Vikhr-Gemma-2B-instruct\"\n",
    "# model_name = \"Vikhrmodels/it-5.4-fp16-orpo-v2\"\n",
    "# model_name = \"IlyaGusev/saiga_llama3_8b\"\n",
    "# model_name = \"lightblue/suzume-llama-3-8B-multilingual-orpo-borda-half\"\n",
    "# model_name = \"lightblue/suzume-llama-3-8B-multilingual\"\n",
    "\n",
    "# model_names = list()\n",
    "# for dirr in os.listdir('./modelcache/'):\n",
    "#     if 'models' in dirr:\n",
    "#         model_name = '/'.join(dirr.split('--')[1:3])\n",
    "#         model_names.append(model_name)\n",
    "# model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f4c20",
   "metadata": {},
   "source": [
    "при порождении для val датасета можно оценить результаты работы модели локально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3190722f-3714-4c59-b7bf-c39c4c572771",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-19T22:26:31.565106Z",
     "iopub.status.busy": "2024-11-19T22:26:31.563532Z",
     "iopub.status.idle": "2024-11-19T23:52:23.004344Z",
     "shell.execute_reply": "2024-11-19T23:52:23.003099Z",
     "shell.execute_reply.started": "2024-11-19T22:26:31.565032Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 22:26:32 config.py:1563] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-19 22:26:32 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='IlyaGusev/saiga_llama3_8b', speculative_config=None, tokenizer='IlyaGusev/saiga_llama3_8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=IlyaGusev/saiga_llama3_8b, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 11-19 22:26:33 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-19 22:26:33 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 22:26:37 model_runner.py:879] Starting to load model IlyaGusev/saiga_llama3_8b...\n",
      "INFO 11-19 22:26:38 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-19 22:26:38 selector.py:116] Using XFormers backend.\n",
      "INFO 11-19 22:26:39 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.65s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.52s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:02,  2.11s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.24s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-19 22:28:55 model_runner.py:890] Loading model weights took 14.9595 GB\n",
      "INFO 11-19 22:28:59 gpu_executor.py:121] # GPU blocks: 5765, # CPU blocks: 2048\n",
      "INFO 11-19 22:29:01 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-19 22:29:01 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-19 22:29:25 model_runner.py:1300] Graph capturing finished in 24 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 22:29:45 scheduler.py:1242] Sequence group 98 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  51%|█████     | 259/512 [02:37<01:50,  2.29it/s, est. speed input: 1492.45 toks/s, output: 420.02 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 22:32:07 scheduler.py:1242] Sequence group 348 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [04:22<00:00,  1.95it/s, est. speed input: 1777.22 toks/s, output: 498.96 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.1 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 52\n",
      "Accuracy: 0.102\n",
      "NaNs: 9\n",
      "0.11352306090157463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  30%|██▉       | 153/512 [01:49<02:02,  2.94it/s, est. speed input: 1480.24 toks/s, output: 357.42 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 22:35:48 scheduler.py:1242] Sequence group 744 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [04:57<00:00,  1.72it/s, est. speed input: 1835.74 toks/s, output: 440.29 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.1 11\n",
      "Count: 68\n",
      "Accuracy: 0.133\n",
      "NaNs: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12291522862448095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12%|█▎        | 64/512 [01:07<07:12,  1.04it/s, est. speed input: 1186.45 toks/s, output: 242.14 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 22:40:01 scheduler.py:1242] Sequence group 1153 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [05:28<00:00,  1.56it/s, est. speed input: 1934.01 toks/s, output: 398.63 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.1 13\n",
      "Count: 65\n",
      "Accuracy: 0.127\n",
      "NaNs: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12471659469773115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 22:44:47 scheduler.py:1242] Sequence group 1598 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [05:44<00:00,  1.49it/s, est. speed input: 2075.36 toks/s, output: 380.70 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.1 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 59\n",
      "Accuracy: 0.115\n",
      "NaNs: 7\n",
      "0.10470942643959763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 22:50:28 scheduler.py:1242] Sequence group 2143 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  53%|█████▎    | 271/512 [02:58<05:20,  1.33s/it, est. speed input: 1399.08 toks/s, output: 389.09 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 22:53:30 scheduler.py:1242] Sequence group 2486 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [04:23<00:00,  1.95it/s, est. speed input: 1788.57 toks/s, output: 497.98 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.2 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 58\n",
      "Accuracy: 0.113\n",
      "NaNs: 3\n",
      "0.10919280246307014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  43%|████▎     | 222/512 [02:37<02:19,  2.08it/s, est. speed input: 1532.12 toks/s, output: 360.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 22:57:14 scheduler.py:1242] Sequence group 2858 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [05:00<00:00,  1.70it/s, est. speed input: 1851.16 toks/s, output: 436.34 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.2 11\n",
      "Count: 66\n",
      "Accuracy: 0.129\n",
      "NaNs: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12289588396936653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  26%|██▋       | 135/512 [01:46<02:19,  2.69it/s, est. speed input: 1542.93 toks/s, output: 324.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:01:32 scheduler.py:1242] Sequence group 3276 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [05:31<00:00,  1.55it/s, est. speed input: 1886.30 toks/s, output: 395.99 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.2 13\n",
      "Count: 72\n",
      "Accuracy: 0.141\n",
      "NaNs: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.133165613095629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█▎        | 66/512 [01:19<11:28,  1.54s/it, est. speed input: 1149.95 toks/s, output: 212.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:06:49 scheduler.py:1242] Sequence group 3770 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [05:41<00:00,  1.50it/s, est. speed input: 2072.54 toks/s, output: 383.28 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.2 15\n",
      "Count: 77\n",
      "Accuracy: 0.150\n",
      "NaNs: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11612862596589649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:11:23 scheduler.py:1242] Sequence group 4185 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  69%|██████▉   | 352/512 [03:24<01:02,  2.56it/s, est. speed input: 1554.98 toks/s, output: 440.02 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:14:21 scheduler.py:1242] Sequence group 4537 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [04:24<00:00,  1.93it/s, est. speed input: 1767.69 toks/s, output: 495.14 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.3 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 65\n",
      "Accuracy: 0.127\n",
      "NaNs: 5\n",
      "0.11914003895746596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  58%|█████▊    | 297/512 [03:17<01:21,  2.65it/s, est. speed input: 1644.46 toks/s, output: 384.51 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:18:42 scheduler.py:1242] Sequence group 4985 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [04:58<00:00,  1.72it/s, est. speed input: 1857.90 toks/s, output: 439.77 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.3 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 77\n",
      "Accuracy: 0.150\n",
      "NaNs: 3\n",
      "0.11822936951656818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  40%|███▉      | 203/512 [02:42<04:38,  1.11it/s, est. speed input: 1536.79 toks/s, output: 319.49 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:23:06 scheduler.py:1242] Sequence group 5387 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [05:30<00:00,  1.55it/s, est. speed input: 1916.35 toks/s, output: 396.63 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.3 13\n",
      "Count: 77\n",
      "Accuracy: 0.150\n",
      "NaNs: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12846402998286416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  24%|██▍       | 123/512 [01:59<05:51,  1.11it/s, est. speed input: 1418.01 toks/s, output: 264.10 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:28:15 scheduler.py:1242] Sequence group 5874 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [05:44<00:00,  1.49it/s, est. speed input: 2057.68 toks/s, output: 380.79 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.3 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 86\n",
      "Accuracy: 0.168\n",
      "NaNs: 4\n",
      "0.1144956821889277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:32:11 scheduler.py:1242] Sequence group 6232 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  68%|██████▊   | 350/512 [03:30<01:25,  1.90it/s, est. speed input: 1516.75 toks/s, output: 424.69 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:35:13 scheduler.py:1242] Sequence group 6580 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [04:23<00:00,  1.94it/s, est. speed input: 1769.94 toks/s, output: 496.64 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.4 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 68\n",
      "Accuracy: 0.133\n",
      "NaNs: 8\n",
      "0.11560101782474515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  59%|█████▉    | 302/512 [03:15<01:06,  3.18it/s, est. speed input: 1660.37 toks/s, output: 395.37 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:39:30 scheduler.py:1242] Sequence group 7036 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [04:59<00:00,  1.71it/s, est. speed input: 1844.72 toks/s, output: 437.59 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.4 11\n",
      "Count: 83\n",
      "Accuracy: 0.162\n",
      "NaNs: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12406213407401646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  40%|███▉      | 203/512 [02:50<06:39,  1.29s/it, est. speed input: 1499.12 toks/s, output: 305.34 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:44:23 scheduler.py:1242] Sequence group 7501 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [05:30<00:00,  1.55it/s, est. speed input: 1906.73 toks/s, output: 397.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.4 13\n",
      "Count: 70\n",
      "Accuracy: 0.137\n",
      "NaNs: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12015075044909788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  24%|██▍       | 122/512 [01:55<05:14,  1.24it/s, est. speed input: 1449.41 toks/s, output: 269.96 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-19 23:48:40 scheduler.py:1242] Sequence group 7859 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 512/512 [05:41<00:00,  1.50it/s, est. speed input: 2066.16 toks/s, output: 384.26 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.4 15\n",
      "Count: 95\n",
      "Accuracy: 0.186\n",
      "NaNs: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12673075957632834\n",
      "CPU times: user 1h 25min 3s, sys: 1min 2s, total: 1h 26min 5s\n",
      "Wall time: 1h 25min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_shots = [9, 11, 13, 15]\n",
    "temperatures = [0.1, 0.2, 0.3, 0.4]\n",
    "model_names = [\"IlyaGusev/saiga_llama3_8b\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    # https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py\n",
    "    llm = LLM(model=model_name, dtype='float16')\n",
    "    \n",
    "    for temperature in temperatures:\n",
    "        # https://docs.vllm.ai/en/latest/getting_started/quickstart.html\n",
    "        # https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py\n",
    "        sampling_params = SamplingParams(temperature=temperature,\n",
    "                                         top_p=0.9,\n",
    "                                         max_tokens = 256,\n",
    "                                         seed = SEED)\n",
    "        \n",
    "        for n_shots in num_shots:\n",
    "            path = get_path(model_name, sampling_params, n_shots)\n",
    "            result = run(llm, sampling_params, train, val, n_shots)\n",
    "            \n",
    "            # output = pd.DataFrame(result, columns = ['sent_id', 'text', 'target', 'pred'])\n",
    "            # save(output, path)\n",
    "            output = pd.DataFrame([(x[0], x[1], x[2], str2list(x[3])) for x in result],\n",
    "                      columns = ['sent_id', 'text', 'target', 'pred'])\n",
    "            print(model_name, temperature, n_shots)\n",
    "            short_report(output)\n",
    "            # save(output, path, raw = False)\n",
    "            output = df2structure(output)\n",
    "            print(f'f1: {do_eval_core(val, output)}')\n",
    "            # save(output, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b92d4",
   "metadata": {},
   "source": [
    "при порождении для test датасета для оценки результатов необходимо отправить .jsonl на Codalab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c45d0295-0f33-45dd-988f-8276825838c0",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-20T00:10:57.039556Z",
     "iopub.status.busy": "2024-11-20T00:10:57.038197Z",
     "iopub.status.idle": "2024-11-20T00:27:58.649512Z",
     "shell.execute_reply": "2024-11-20T00:27:58.648363Z",
     "shell.execute_reply.started": "2024-11-20T00:10:57.039519Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-20 00:10:57 config.py:1563] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-20 00:10:57 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='IlyaGusev/saiga_llama3_8b', speculative_config=None, tokenizer='IlyaGusev/saiga_llama3_8b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=IlyaGusev/saiga_llama3_8b, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 11-20 00:10:59 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-20 00:10:59 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 00:11:01 model_runner.py:879] Starting to load model IlyaGusev/saiga_llama3_8b...\n",
      "INFO 11-20 00:11:01 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-20 00:11:01 selector.py:116] Using XFormers backend.\n",
      "INFO 11-20 00:11:02 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.64s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.52s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:02,  2.01s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.23s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 00:13:22 model_runner.py:890] Loading model weights took 14.9595 GB\n",
      "INFO 11-20 00:13:26 gpu_executor.py:121] # GPU blocks: 5765, # CPU blocks: 2048\n",
      "INFO 11-20 00:13:28 model_runner.py:1181] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-20 00:13:28 model_runner.py:1185] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-20 00:13:52 model_runner.py:1300] Graph capturing finished in 24 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1316 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-20 00:14:18 scheduler.py:1242] Sequence group 70 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  25%|██▌       | 335/1316 [04:05<13:46,  1.19it/s, est. speed input: 1687.98 toks/s, output: 349.80 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-20 00:18:07 scheduler.py:1242] Sequence group 399 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  61%|██████    | 806/1316 [08:49<03:04,  2.77it/s, est. speed input: 1864.42 toks/s, output: 389.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-20 00:22:50 scheduler.py:1242] Sequence group 878 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  91%|█████████▏| 1204/1316 [13:10<00:50,  2.23it/s, est. speed input: 1875.81 toks/s, output: 389.90 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-20 00:27:09 scheduler.py:1242] Sequence group 1275 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1316/1316 [14:00<00:00,  1.57it/s, est. speed input: 1927.63 toks/s, output: 400.85 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IlyaGusev/saiga_llama3_8b 0.2 13\n",
      "results/saiga_llama3_8b/saiga_llama3_8b_bl_13shot_0.2temp\n",
      "CPU times: user 15min 34s, sys: 49.7 s, total: 16min 24s\n",
      "Wall time: 17min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_shots = [13]\n",
    "temperatures = [0.2]\n",
    "model_names = [\"IlyaGusev/saiga_llama3_8b\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    # https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py\n",
    "    llm = LLM(model=model_name, dtype='float16')\n",
    "    \n",
    "    for temperature in temperatures:\n",
    "        # https://docs.vllm.ai/en/latest/getting_started/quickstart.html\n",
    "        # https://github.com/vllm-project/vllm/blob/main/vllm/sampling_params.py\n",
    "        sampling_params = SamplingParams(temperature=temperature,\n",
    "                                         top_p=0.9,\n",
    "                                         max_tokens = 256,\n",
    "                                         seed = SEED)\n",
    "        \n",
    "        for n_shots in num_shots:\n",
    "            path = get_path(model_name, sampling_params, n_shots)\n",
    "            result = run(llm, sampling_params, train, test, n_shots)\n",
    "            output = pd.DataFrame([(x[0], x[1], x[2], str2list(x[3])) for x in result],\n",
    "                      columns = ['sent_id', 'text', 'target', 'pred'])\n",
    "            output = df2structure(output)\n",
    "            print(model_name, temperature, n_shots)\n",
    "            print(path)\n",
    "            save(output, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
